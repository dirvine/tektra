[package]
name = "tektra-inference"
version = "0.1.0"
edition = "2021"
description = "Tektra's custom inference engine based on mistral.rs"
license = "MIT OR Apache-2.0"
repository = "https://github.com/dirvine/tektra"

[dependencies]
# Core ML Framework
tektra-candle = { path = "../candle/candle-core", default-features = false, features = ["metal"] }
candle-nn = { path = "../candle/candle-nn" }
candle-transformers = { path = "../candle/candle-transformers" }

# Tokenization
tektra-tokenizers = { path = "../tokenizers-custom" }

# Model Management
tektra-models = { path = "../hf-hub-tektra" }
safetensors = "0.4"

# Async Support
tokio = { version = "1.0", features = ["full"] }
tokio-stream = "0.1"
async-trait = "0.1"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Error Handling
anyhow = "1.0"
thiserror = "1.0"

# Logging
tracing = "0.1"

# Utilities
parking_lot = "0.12"
indexmap = "2.0"
either = "1.9"

# Platform-specific optimizations
[target.'cfg(target_os = "macos")'.dependencies]
accelerate-src = "0.3"

[features]
default = ["metal"]
metal = ["tektra-candle/metal"]
cuda = ["tektra-candle/cuda"]
cpu-only = []
vision = ["candle-transformers/vision"]
streaming = []
profiling = []

# Development features
dev = ["profiling", "streaming"]
full = ["metal", "vision", "streaming", "profiling"]