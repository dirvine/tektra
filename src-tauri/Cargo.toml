[package]
name = "tektra"
version = "0.1.0"
description = "A voice-interactive AI assistant with multimodal capabilities"
authors = ["David Irvine <david.irvine@maidsafe.net>"]
license = "MIT OR Apache-2.0"
repository = "https://github.com/dirvine/tektra"
homepage = "https://github.com/dirvine/tektra"
readme = "../README.md"
keywords = ["ai", "assistant", "voice", "tauri", "gemma"]
categories = ["command-line-utilities", "multimedia::audio"]
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[build-dependencies]
tauri-build = { version = "1.5", features = [] }

[dependencies]
tauri = { version = "1.6", features = [ "fs-read-dir", "fs-rename-file", "fs-write-file", "fs-read-file", "http-all", "dialog-ask", "fs-copy-file", "notification-all", "dialog-open", "dialog-save", "fs-create-dir", "fs-exists", "dialog-confirm", "fs-remove-dir", "shell-open", "dialog-message", "fs-remove-file"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tokio = { version = "1", features = ["full"] }

# ML Dependencies for TinyLlama
hf-hub = { version = "0.3", features = ["tokio"] }
anyhow = "1.0"
tracing = "0.1"
tracing-subscriber = "0.3"
reqwest = { version = "0.11", features = ["json", "stream"] }
dirs = "5.0"
futures = "0.3"
bytesize = "1.3"
# Audio processing
cpal = "0.15"
hound = "3.5"
# rubato = "0.15"  # For resampling if needed later

# Speech processing
# whisper-rs = "0.11"  # For Whisper STT when ready
# ort = "2.0"  # ONNX Runtime for Silero VAD when ready

# Tokenizer support for both GGUF and MLX
tokenizers = { version = "0.20", features = ["onig"] }

# Model inference backends
[target.'cfg(target_os = "macos")'.dependencies]
# MLX for Apple Silicon (much faster on M1/M2/M3)
# NOTE: Requires XCode Command Line Tools and metal compiler
# mlx-rs = { version = "0.25.0-alpha.1", features = ["metal", "accelerate"], optional = true }

# GGUF model inference (cross-platform fallback)
# TODO: Add proper GGUF inference library when available
# Options: llama-cpp-rs, candle, or custom bindings

# For downloading models
# (already have hf-hub, reqwest, futures, dirs)

# For camera/video input (temporarily disabled - using test mode)
# nokhwa = { version = "0.10", features = ["input-avfoundation"], default-features = false }

# For avatar rendering (3D - not needed for 2D canvas)
# wgpu = "0.20"

# For base64 encoding
base64 = "0.22"

# For image encoding
png = "0.17"

[[bin]]
name = "tektra"
path = "src/main.rs"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
strip = true
