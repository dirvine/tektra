# Tektra Model Configuration
# This file defines model configurations and backend preferences

[backends]
# Default backend preferences (order matters)
default = ["enhanced_ollama", "ollama"]

[backends.enhanced_ollama]
enabled = true
host = "localhost"
port = 11434
timeout_seconds = 300

[backends.ollama]
enabled = true
host = "localhost"
port = 11434
timeout_seconds = 300

[backends.mistral_rs]
enabled = false  # Disabled until dependency issues are resolved
flash_attention = true
mcp_enabled = true
device = "auto"

[backends.llama_cpp]
enabled = false  # Disabled until dependency issues are resolved
n_threads = 8
use_mmap = true
n_gpu_layers = -1
device = "auto"

# Model definitions
[[models]]
id = "gemma3n:e4b"
name = "Gemma 3N E4B"
description = "Gemma 3N with enhanced 4-bit quantization for robotics"
backend_preferences = ["enhanced_ollama", "ollama"]
template = "gemma"
context_length = 32768
multimodal = true
capabilities = ["text", "image", "audio"]

[[models]]
id = "gemma2:2b"
name = "Gemma 2 2B"
description = "Lightweight Gemma 2 model"
backend_preferences = ["ollama", "mistral_rs", "llama_cpp"]
template = "gemma"
context_length = 8192
multimodal = false
capabilities = ["text"]

[[models]]
id = "llama3.2-vision:11b"
name = "Llama 3.2 Vision 11B"
description = "Multimodal Llama model with vision capabilities"
backend_preferences = ["ollama", "mistral_rs"]
template = "llama"
context_length = 128000
multimodal = true
capabilities = ["text", "image"]

[[models]]
id = "qwen2.5:7b"
name = "Qwen 2.5 7B"
description = "Qwen 2.5 model with strong reasoning"
backend_preferences = ["ollama", "mistral_rs", "llama_cpp"]
template = "qwen"
context_length = 32768
multimodal = false
capabilities = ["text"]

# Generation presets
[presets.default]
max_tokens = 512
temperature = 0.7
top_p = 0.9
top_k = 40
repeat_penalty = 1.1

[presets.creative]
max_tokens = 1024
temperature = 0.9
top_p = 0.95
top_k = 100
repeat_penalty = 1.0

[presets.precise]
max_tokens = 256
temperature = 0.3
top_p = 0.8
top_k = 20
repeat_penalty = 1.2

# Memory limits (in MB)
[memory_limits]
mistral_rs = 8192
llama_cpp = 4096
ollama = 2048
total = 16384

# MCP (Model Context Protocol) Configuration
[mcp]
enabled = true
server_port = 8765
protocol_version = "2024-11-05"

[mcp.resources]
# File system resources
workspace_documents = { uri = "file:///workspace/documents", enabled = true }
project_files = { uri = "file:///workspace/src", enabled = true }
test_data = { uri = "file:///workspace/test_data", enabled = true }

# Memory resources
conversation_memory = { uri = "memory://conversation", enabled = true }
document_cache = { uri = "memory://document_cache", enabled = true }

[mcp.tools]
# Document processing tools
search_documents = { enabled = true, max_results = 10 }
analyze_image = { enabled = true, supported_formats = ["png", "jpg", "webp"] }
process_audio = { enabled = true, max_duration_seconds = 300 }
extract_text = { enabled = true, formats = ["pdf", "docx", "txt", "md"] }

# AI assistance tools
generate_code = { enabled = true, languages = ["rust", "typescript", "python"] }
explain_concept = { enabled = true }
translate_text = { enabled = true, languages = ["en", "es", "fr", "de", "zh", "ja"] }

[mcp.prompts]
# Predefined prompt templates
code_review = { enabled = true, languages = ["rust", "typescript", "python", "go"] }
document_qa = { enabled = true }
summarize = { enabled = true, styles = ["brief", "detailed", "bullets"] }
debug_help = { enabled = true }
api_design = { enabled = true }

[mcp.client]
# External MCP servers to connect to
anthropic_claude = { url = "https://mcp.anthropic.com", enabled = false }
local_knowledge_base = { url = "http://localhost:9000", enabled = false }

[mcp.security]
# Security settings
require_auth = false
allowed_origins = ["http://localhost:*", "tauri://localhost"]
max_request_size_mb = 50
rate_limit_per_minute = 100