# Tektra Model Configuration
# Primary model configurations for multimodal AI capabilities

[models.qwen25_omni]
name = "Qwen2.5-Omni 7B"
model_id = "Qwen/Qwen2.5-Omni-7B"
quantization = "Q6_K"
context_window = 32768
supports_vision = true
supports_audio = true
supports_speech_output = true
supports_real_time = true
supports_documents = true
supports_video = true
thinker_talker_architecture = true
default = true
description = "Latest multimodal model with real-time audio processing and speech synthesis"
memory_requirement_gb = 6
download_size_gb = 4.2

# Omni-specific configuration
[models.qwen25_omni.audio_config]
sample_rate = 16000
chunk_size = 1024
max_audio_length_seconds = 30
enable_vad = true
enable_streaming = true
speech_synthesis_voice = "default"

[models.qwen25_omni.vision_config]
max_image_size = 2048
supports_video_frames = true
temporal_understanding = true
ocr_enabled = true

[models.qwen25_omni.performance]
batch_size = 1
max_sequence_length = 32768
use_flash_attention = true
enable_kv_cache = true
low_memory_mode = false

# Keep VL as fallback option
[models.qwen25_vl]
name = "Qwen2.5-VL 7B"
model_id = "Qwen/Qwen2.5-VL-7B-Instruct"
quantization = "Q6_K"
context_window = 32768
supports_vision = true
supports_audio = false
supports_speech_output = false
supports_real_time = false
supports_documents = true
supports_video = false
thinker_talker_architecture = false
default = false
description = "Vision-language model for image and document analysis"
memory_requirement_gb = 5
download_size_gb = 3.8

[models.qwen25_vl.vision_config]
max_image_size = 2048
supports_video_frames = false
temporal_understanding = false
ocr_enabled = true

[models.qwen25_vl.performance]
batch_size = 1
max_sequence_length = 32768
use_flash_attention = true
enable_kv_cache = true
low_memory_mode = false

# Additional model options
[models.llama32_vision]
name = "Llama 3.2 Vision 11B"
model_id = "meta-llama/Llama-3.2-11B-Vision-Instruct"
quantization = "Q4_K"
context_window = 8192
supports_vision = true
supports_audio = false
supports_speech_output = false
supports_real_time = false
default = false
description = "Meta's vision-language model"
memory_requirement_gb = 8
download_size_gb = 6.2

[models.pixtral]
name = "Pixtral 12B"
model_id = "mistralai/Pixtral-12B-2409"
quantization = "Q4_K"
context_window = 32768
supports_vision = true
supports_audio = false
supports_speech_output = false
supports_real_time = false
default = false
description = "Mistral's vision-language model"
memory_requirement_gb = 9
download_size_gb = 7.1

# Global configuration
[global]
default_model = "qwen25_omni"
auto_download = true
cache_directory = "~/.tektra/models"
max_concurrent_downloads = 2
enable_model_switching = true
preload_models = ["qwen25_omni"]

[global.hardware]
prefer_gpu = true
gpu_memory_fraction = 0.8
enable_mixed_precision = true
optimize_for_latency = true

[global.audio]
default_sample_rate = 16000
default_channels = 1
enable_noise_reduction = true
enable_echo_cancellation = true
real_time_buffer_size = 4096

[global.vision]
default_image_format = "RGB"
auto_resize_images = true
preserve_aspect_ratio = true
enable_image_enhancement = true

[global.conversation]
enable_memory = true
max_history_length = 50
auto_summarize_threshold = 30
enable_context_compression = true