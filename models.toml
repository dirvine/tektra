# Tektra Model Configuration
# This file defines model configurations and backend preferences

[backends]
# Primary backend preferences in order
default = ["mistral_rs", "llama_cpp", "ollama"]

# Backend-specific configurations
[backends.mistral_rs]
enabled = true
flash_attention = true
mcp_enabled = true
device = "auto"  # auto, cpu, cuda:0, metal

[backends.llama_cpp]
enabled = true
n_threads = 8
use_mmap = true
n_gpu_layers = -1  # -1 for all layers on GPU
device = "auto"

[backends.ollama]
enabled = true
host = "localhost"
port = 11434
timeout_seconds = 300

# Model definitions
[[models]]
id = "gemma3n:e4b"
name = "Gemma 3N E4B"
description = "Multimodal Gemma 3N model with text, image, audio, and video support"
backend_preferences = ["ollama", "mistral_rs"]
template = "gemma"
context_length = 8192
multimodal = true
capabilities = ["text", "image", "audio", "video"]

[[models]]
id = "gemma2:2b"
name = "Gemma 2 2B"
description = "Small, fast Gemma 2 model for quick responses"
backend_preferences = ["mistral_rs", "llama_cpp"]
template = "gemma"
context_length = 8192
multimodal = false
capabilities = ["text"]

[[models]]
id = "llama3.2:3b"
name = "Llama 3.2 3B"
description = "Latest Llama model with improved reasoning"
backend_preferences = ["mistral_rs", "llama_cpp", "ollama"]
template = "llama"
context_length = 131072
multimodal = false
capabilities = ["text"]

[[models]]
id = "llama3.2-vision:11b"
name = "Llama 3.2 Vision 11B"
description = "Multimodal Llama with vision capabilities"
backend_preferences = ["mistral_rs", "ollama"]
template = "llama"
context_length = 131072
multimodal = true
capabilities = ["text", "image"]

[[models]]
id = "mistral:7b"
name = "Mistral 7B"
description = "Efficient Mistral model with strong performance"
backend_preferences = ["mistral_rs", "llama_cpp", "ollama"]
template = "mistral"
context_length = 32768
multimodal = false
capabilities = ["text"]

[[models]]
id = "mixtral:8x7b"
name = "Mixtral 8x7B"
description = "MoE model with excellent performance"
backend_preferences = ["mistral_rs", "ollama"]
template = "mistral"
context_length = 32768
multimodal = false
capabilities = ["text"]

[[models]]
id = "phi3:medium"
name = "Phi 3 Medium"
description = "Microsoft's efficient Phi 3 model"
backend_preferences = ["mistral_rs", "llama_cpp", "ollama"]
template = "phi"
context_length = 131072
multimodal = false
capabilities = ["text"]

[[models]]
id = "qwen2.5:7b"
name = "Qwen 2.5 7B"
description = "Alibaba's multilingual model"
backend_preferences = ["mistral_rs", "llama_cpp", "ollama"]
template = "qwen"
context_length = 131072
multimodal = false
capabilities = ["text"]

[[models]]
id = "llava:7b"
name = "LLaVA 7B"
description = "Vision-language model based on Llama"
backend_preferences = ["llama_cpp", "ollama"]
template = "chatml"
context_length = 4096
multimodal = true
capabilities = ["text", "image"]

# Quantized model configurations
[[models]]
id = "models/mlx-community/Qwen2.5-7B-Instruct-4bit"
name = "Qwen 2.5 7B Instruct (4-bit)"
description = "Quantized Qwen model for Apple Silicon"
backend_preferences = ["mistral_rs"]
template = "qwen"
context_length = 131072
multimodal = false
capabilities = ["text"]
quantization = "Q4_K_M"
device = "metal"

[[models]]
id = "models/TheBloke/Llama-2-7B-Chat-GGUF"
name = "Llama 2 7B Chat (GGUF)"
description = "GGUF format Llama 2 for llama.cpp"
backend_preferences = ["llama_cpp"]
template = "llama"
context_length = 4096
multimodal = false
capabilities = ["text"]
model_file = "llama-2-7b-chat.Q4_K_M.gguf"

# Custom model paths (for local models)
[custom_models]
# Example: my_model = "/path/to/model/directory"

# Performance presets
[presets.fast]
max_tokens = 256
temperature = 0.7
top_p = 0.9
top_k = 40
repeat_penalty = 1.1

[presets.balanced]
max_tokens = 512
temperature = 0.8
top_p = 0.95
top_k = 50
repeat_penalty = 1.1

[presets.quality]
max_tokens = 2048
temperature = 0.9
top_p = 0.95
top_k = 100
repeat_penalty = 1.05

[presets.creative]
max_tokens = 1024
temperature = 1.2
top_p = 0.98
top_k = 200
repeat_penalty = 1.0

# Memory limits per backend
[memory_limits]
mistral_rs = 16384  # 16GB
llama_cpp = 12288   # 12GB
ollama = 8192       # 8GB
total = 32768       # 32GB total