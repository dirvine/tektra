# file: /Users/davidirvine/Desktop/Devel/projects/tektra/src/tektra/ai/qwen_backend.py
# hypothesis_version: 6.135.26

[0.0, 0.3, 0.7, 0.9, 8.0, -100, 100, 1024, 2048, 1000000, '--- End Context ---\n', '-VL', '2.5-VL', 'Loading model...', 'Loading tokenizer...', 'Not started', 'RGB', 'VL', 'agent_id', 'auto', 'avg_inference_time', 'avg_memory_mb', 'cpu', 'cuda', 'current_memory_mb', 'default', 'device', 'do_sample', 'eos_token_id', 'flash_attention_2', 'float16', 'generated_text', 'input', 'input_ids', 'is_initialized', 'is_vision_model', 'last_inference_time', 'loading_progress', 'loading_status', 'max_inference_time', 'max_new_tokens', 'min_inference_time', 'model_name', 'mps', 'output', 'pad_token_id', 'parameter_count', 'performance_stats', 'pt', 'quantization_bits', 'session_id', 'temperature', 'text-generation', 'to', 'tokens_per_second', 'top_p', 'total_inferences', 'total_input_tokens', 'total_output_tokens', 'unknown', 'user_id', 'vision']